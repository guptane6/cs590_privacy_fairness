{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import model_bias_analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina' # feel free to comment this line out if not using retina screen\n",
    "import CommonFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_comments, test_comments, i):\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "        ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "        ('clf', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "    clf = clf.fit(train_comments['comment'], train_comments['binary_tox'])\n",
    "    auc = roc_auc_score(test_comments['binary_tox'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    \n",
    "    test_comments[\"predicted\"+str(i)] = clf.predict(test_comments['comment'])\n",
    "    \n",
    "    return test_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logDI(df, labels_col, terms):\n",
    "    # labels_col is true/false of the comment being classified as toxic. ('binary_tox' I believe)\n",
    "    # terms should be the array of top 8 terms\n",
    "    \n",
    "    logDI_arr = np.empty(((len(terms)), len(terms)))\n",
    "    \n",
    "    for i in range(len(terms)):\n",
    "        for j in range(len(terms)):\n",
    "            print(\"******\", i)\n",
    "            logDI_arr[i, j] =  (math.log(len(df[(df[terms[i]]==True) & (df[labels_col]==True)]) / len(df[df[terms[i]]==True])) \\\n",
    "                            - math.log(len(df[(df[terms[j]]==True) & (df[labels_col]==True)]) / len(df[df[terms[j]]==True])))**2\n",
    "                \n",
    "    logDI = logDI_arr.sum() / 2\n",
    "    return logDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "50 25.29190818202314\n",
      "Test ROC AUC: 0.953\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 0\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 1\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 2\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 3\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 4\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 5\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 6\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "****** 7\n",
      "50 35.812539632135554\n"
     ]
    }
   ],
   "source": [
    "madlibs_terms = ['gay', 'homosexual', 'straight', 'black', 'white', 'american', 'jewish', 'old']\n",
    "dict_train_logs = {}\n",
    "dict_test_logs = {}\n",
    "array_predicted_bin_tox = []\n",
    "array_test_bin_tox = []\n",
    "\n",
    "#TODO: This code is really unnecessarily complicated, you could instead have a dictionary of dataframes and store all\n",
    "for i in range(50, 55, 5):\n",
    "# for i in range(30,40,5):\n",
    "    joined_tox = pd.read_csv('joined_tox' + str(i) + '.csv')\n",
    "    train_comments = pd.read_csv('train_comments'+ str(i) + '.csv')\n",
    "    test_comments = pd.read_csv('test_comments' + str(i) + '.csv')\n",
    "    # madlibs_terms = model_bias_analysis.read_identity_terms('test_comments'+str(i)+'.csv')\n",
    "    \n",
    "    log_val_train = logDI(train_comments, 'binary_tox', madlibs_terms)\n",
    "    print(str(i), log_val_train)\n",
    "    dict_train_logs[i] = log_val_train\n",
    "    \n",
    "    test_comments = train_and_predict(train_comments, test_comments,str(i))\n",
    "    \n",
    "#     temp_array = []\n",
    "#     temp_array = test_comments['predicted'+str(i)]\n",
    "#     array_predicted_bin_tox.append(temp_array)\n",
    "    \n",
    "#     temp_array = []\n",
    "#     temp_array = test_comments['binary_tox']\n",
    "#     array_test_bin_tox.append(temp_array)\n",
    "    \n",
    "    log_val_test = logDI(test_comments, 'predicted'+str(i), madlibs_terms)\n",
    "    print(str(i), log_val_test)\n",
    "    dict_test_logs[i] = log_val_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
