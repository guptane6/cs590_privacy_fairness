{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Much of this code was lifted from [the Conversation AI project](https://conversationai.github.io/). In this file, instead of using max DI I want to do each of the DIs against gay and basically plot a bunch of lines on the same axis, di of input and di of output but one line for white, one line for black, one line for jewish etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import model_bias_analysis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read TSVs from file. These are the original data from *Conversation AI*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *grouped_annotations* takes the mean of all toxicity ratings of a comment.\n",
    "* *joined_tox* joins *grouped_annotations* and *comments*.\n",
    "* We also add a column *binary_tox* to the dataframe *joined_tox*. Here we assign a toxicity rating of 0 or 1 based on whether the mean toxicity rating is above or below 0.5 (or other value determined by threshold_binary_toxicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation DI:\n",
    "DI(t1,t2) = (probability that comment containing term t1 is labeled toxic) / (probability that comment containing term t2 is labelled toxic)\n",
    "= a/b\n",
    "\n",
    "a = # comments containing t1 AND toxic / # comments containing t1\n",
    "= alpha/beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_di(df, madlibs_terms, colname):\n",
    "    \n",
    "    # We now calculate the DI for each pair for the training data.\n",
    "\n",
    "    term_toxicity = np.zeros(len(madlibs_terms))\n",
    "\n",
    "    # we temporarily put -1 in as a placeholder for stuff that has no exacmples of terms with that term.\n",
    "    # non-binary is one\n",
    "    for i in range(len(madlibs_terms)):\n",
    "        try:\n",
    "            term_toxicity[i] = float((df[(df[madlibs_terms[i]] == True) & (df[colname] == 1)].shape[0]))/  \\\n",
    "            float((df[df[madlibs_terms[i]] == True].shape[0]))\n",
    "        except ZeroDivisionError:\n",
    "            term_toxicity[i] = -1 \n",
    "\n",
    "    return term_toxicity\n",
    "\n",
    "# term_toxicity = calculate_pairwise_di(train_comments, madlibs_terms, 'binary_tox')\n",
    "# print(term_toxicity)\n",
    "# print(len(term_toxicity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds the max DI between all pairs of identity terms, which for one of our experiments,\n",
    "# we're claiming is kind of the DI of all terms\n",
    "# Possibly something we'll take out or come back to once we try more versions of our code. (Neha's working on this!)\n",
    "\n",
    "def gay_ratio(term_toxicity, madlibs_terms):\n",
    "    term_vs_gay = []\n",
    "    top_indexes = ()\n",
    "    j = 0 # This is the index of gay\n",
    "    for i in range(len(madlibs_terms)):\n",
    "        if (i==j or term_toxicity[i] == -1 or term_toxicity[j] == -1 or term_toxicity[j] == 0):\n",
    "            term_vs_gay.append(1)\n",
    "        else:\n",
    "            term_vs_gay.append(term_toxicity[i]/ term_toxicity[j])\n",
    "                \n",
    "    return term_vs_gay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate max DIs on all of the perturbed training datasets\n",
    "def find_gay_dis(df_comments, madlibs_terms, colname):\n",
    "    \n",
    "    term_vs_gay = []\n",
    "    # So this is a series of DIs, one is t1/gay di, t2/gay di...\n",
    "\n",
    "    # df_comments['newcol'] = list_perturbation[i]\n",
    "    term_toxicity = calculate_pairwise_di(df_comments, madlibs_terms, colname)\n",
    "    term_vs_gay = gay_ratio(term_toxicity, madlibs_terms)\n",
    "        \n",
    "    # This is the array of all of the max_dis, and the indexes of the madlibs_terms array that composed that max_di\n",
    "    return term_vs_gay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_comments, test_comments):\n",
    "\n",
    "#     # This trains a classifier on n different perturbed datsets\n",
    "#     d={}\n",
    "#     for x in range(NUM_PERTURBATIONS):\n",
    "#         d[\"clf{0}\".format(x)] = Pipeline([\n",
    "#             ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "#             ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "#             ('clf', LogisticRegression()),\n",
    "#         ])\n",
    "#         d[\"clf{0}\".format(x)] = d[\"clf{0}\".format(x)].\\\n",
    "#                                     fit(train_comments['comment'], list_perturbations_training[x])\n",
    "#         d[\"auc{0}\".format(x)] = roc_auc_score(test_comments['binary_tox'], \\\n",
    "#                                     d[\"clf{0}\".format(x)].predict_proba(test_comments['comment'])[:, 1])\n",
    "#         print('x Test ROC AUC: %.5f' %d[\"auc{0}\".format(x)])\n",
    "        \n",
    "#     # Once a classifier is trained, this goes to the test data and creates predictions on test data\n",
    "#     perturbed_predictions = [] # list, each item is array of predictions. element 0 is 0th perturbation and \n",
    "#     # predictions based on that.\n",
    "#     # each item in the array is a column that indicates 0/1 for predicted not-toxic/toxic\n",
    "\n",
    "#     for i in range(NUM_PERTURBATIONS):\n",
    "#         perturbed_predictions.append(d[\"clf{0}\".format(i)].predict(test_comments['comment']))\n",
    "                                                                                 \n",
    "#     return perturbed_predictions\n",
    "\n",
    "    clf = Pipeline([\n",
    "        ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "        ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "        ('clf', LogisticRegression()),\n",
    "    ])\n",
    "\n",
    "    clf = clf.fit(train_comments['comment'], train_comments['binary_tox'])\n",
    "    auc = roc_auc_score(test_comments['binary_tox'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "    print('Test ROC AUC: %.3f' %auc)\n",
    "    \n",
    "    test_comments[\"predicted\"] = clf.predict(test_comments['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 [1, 0.8863430810973771, 0.37764638565706665, 0.4076357128653808, 0.3315111973196967, 0.2503615469544529, 0.3551835548713056, 0.2745952193094558]\n",
      "Test ROC AUC: 0.953\n",
      "30 [1, 0.8425760286225403, 0.3087512291052114, 0.27799911465250116, 0.24236605293874017, 0.17018970189701899, 0.24326485729994501, 0.18063773833004604]\n",
      "35 [1, 0.7782363345405542, 0.29839612085042894, 0.3733125937447919, 0.30149268436868815, 0.19097848233796585, 0.2713950797525943, 0.23209966998134135]\n"
     ]
    }
   ],
   "source": [
    "madlibs_terms = ['gay', 'homosexual', 'straight', 'black', 'white', 'american', 'jewish', 'old']\n",
    "array_of_train_dis = []\n",
    "array_of_test_dis = []\n",
    "\n",
    "for i in range(30,95,5):\n",
    "    joined_tox = pd.read_csv('joined_tox'+str(i)+'.csv')\n",
    "    train_comments = pd.read_csv('train_comments'+str(i)+'.csv')\n",
    "    test_comments = pd.read_csv('test_comments.csv')\n",
    "    # madlibs_terms = model_bias_analysis.read_identity_terms('test_comments'+str(i)+'.csv')\n",
    "    \n",
    "    training_data_dis = find_gay_dis(train_comments, madlibs_terms, 'binary_tox')\n",
    "    print(str(i), training_data_dis)\n",
    "    array_of_train_dis.append((i,training_data_dis))\n",
    "    \n",
    "    train_and_predict(train_comments, test_comments)\n",
    "    \n",
    "    test_data_dis = find_gay_dis(test_comments, madlibs_terms, 'predicted')\n",
    "    print(str(i), test_data_dis)\n",
    "    array_of_test_dis.append((i, test_data_dis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A graph of the difference between test and training DI on y axis and x axis of threshold\n",
    "# for t1/gay\n",
    "x_axis = []\n",
    "y_axis_t1 = []\n",
    "\n",
    "for i in range(30,95,5):\n",
    "    x_axis.append(i)\n",
    "\n",
    "y_axis_t1 = []\n",
    "y_axis_t2 = []\n",
    "y_axis_t5 = []\n",
    "for i in range(len(array_of_test_dis)):\n",
    "    training_di = array_of_train_dis[i][1][1]\n",
    "    test_di = array_of_test_dis[i][1][1]\n",
    "    y_axis_t1.append(training_di - test_di)\n",
    "    y_axis_t2.append(array_of_train_dis[i][1][2] - array_of_test_dis[i][1][2])\n",
    "    y_axis_t5.append(array_of_train_dis[i][1][5] - array_of_test_dis[i][1][5])\n",
    "plt.plot(x_axis, y_axis_t1, \"ro\")\n",
    "plt.plot(x_axis, y_axis_t2, \"bo\")\n",
    "plt.plot(x_axis, y_axis_t5, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis_t1 = []\n",
    "y_axis_t5 = []\n",
    "for i in range(len(array_of_test_dis)):\n",
    "    y_axis_t1.append(array_of_test_dis[i][1][1])\n",
    "    y_axis_t5.append(array_of_test_dis[i][1][5])\n",
    "plt.plot(x_axis, y_axis_t1, \"ro\")\n",
    "plt.plot(x_axis, y_axis_t5, \"go\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_axis_t1 = []\n",
    "y_axis_t5 = []\n",
    "for i in range(len(array_of_test_dis)):\n",
    "    y_axis_t1.append(array_of_train_dis[i][1][1])\n",
    "    y_axis_t5.append(array_of_train_dis[i][1][5])\n",
    "plt.plot(x_axis, y_axis_t1, \"ro\")\n",
    "plt.plot(x_axis, y_axis_t5, \"go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
