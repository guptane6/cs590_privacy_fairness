{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commented out because I believe we're not using these files right now. \n",
    "\n",
    "# toxicity_debiasing_data = pd.read_csv(\"toxicity_debiasing_data.tsv\", sep='\\t')\n",
    "# # review_id, comment, is toxic or not, split (train or test)\n",
    "\n",
    "# toxicity_debiasing_data_random = pd.read_csv(\"toxicity_debiasing_data_random.tsv\", sep = '\\t')\n",
    "# # review_id, comment, is toxic or not, split (train or test)\n",
    "\n",
    "# wiki_debias_dev = pd.read_csv(\"wiki_debias_dev.csv\") \n",
    "# # comment,is_toxic,logged_in,ns,rev_id,sample,split,toxicity,year\n",
    "\n",
    "# wiki_debias_random_dev = pd.read_csv(\"wiki_debias_random_dev.csv\")\n",
    "# # comment,is_toxic,logged_in,ns,rev_id,sample,split,toxicity,year\n",
    "\n",
    "# wiki_debias_random_test = pd.read_csv(\"wiki_debias_random_test.csv\")\n",
    "# # comment,is_toxic,logged_in,ns,rev_id,sample,split,toxicity,year\n",
    "\n",
    "#toxicity_debiasing_data.shape#.head\n",
    "#toxicity_debiasing_data_random.shape\n",
    "#wiki_debias_dev.shape\n",
    "#wiki_debias_random_dev.shape\n",
    "#wiki_debias_random_test.shape\n",
    "#sum(toxicity_debiasing_data['split'] == \"test\")/sum(toxicity_debiasing_data['split'] == \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set (https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973) includes over 100k labeled discussion comments from English Wikipedia. Each comment was labeled by multiple annotators via Crowdflower on whether it is a toxic or healthy contribution. We also include some demographic data for each crowd-worker. See our wiki for documentation of the schema of each file and our research paper for documentation on the data collection and modeling methodology. For a quick demo of how to use the data for model building and analysis, check out this ipython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"toxicity_annotated_comments.tsv\"\\\n",
    "                                          , sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from documentation: <br>\n",
    "\"Schema for {attack/aggression/toxicity}_annotated_comments.tsv\n",
    "The comment text and metadata for comments with attack/aggression/toxicity labels generated by crowd-workers. The actual labels are in the corresponding {attack/aggression/toxicity}_annotations.tsv since each comment was labeled multiple times.\n",
    "\n",
    "rev_id: MediaWiki revision id of the edit that added the comment to a talk page (i.e. discussion). <br>\n",
    "comment: Comment text. Consists of the concatenation of content added during a revision/edit of a talk page. MediaWiki markup and HTML have been stripped out. To simplify tsv parsing, \\n has been mapped to NEWLINE_TOKEN, \\t has been mapped to TAB_TOKEN and \" has been mapped to `. <br>\n",
    "year: The year the comment was posted in. <br>\n",
    "logged_in: Indicator for whether the user who made the comment was logged in. Takes on values in {0, 1}. <br>\n",
    "ns: Namespace of the discussion page the comment was made in. Takes on values in {user, article}. <br>\n",
    "sample: Indicates whether the comment came via random sampling of all comments, or whether it came from random sampling of the 5 comments around a block event for violating WP:npa or WP:HA. Takes on values in {random, blocked}. <br>\n",
    "split: For model building in our paper we split comments into train, dev and test sets. Takes on values in {train, dev, test}.\"\n",
    "<br>\n",
    "\n",
    "My notes: <br> \n",
    "I don't know enough about how natural language processing works, but from the snippets that I do know, I imagine that the really really long comments probably aren't very good at being classified even. I also wonder about how bigram toxicity works and whether this is something the training data accounts for (eg \"nasty woman\" vs \"nasty\" has different sources of problems). What are they classifying on/why does this work? We can see Figure 1 in the Dixon paper for comment length and should be able to filter there. I wonder if the phrase templating of classification works for the problems that I raise of large comment length and bigrams. Is this an issue we should be dealing with?\n",
    "Also, an easier couple of questions are: what does the ns and sample really mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(\"toxicity_annotations.tsv\"\\\n",
    "                                          , sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from documentation:\n",
    "        Schema for toxicity_annotations.tsv\n",
    "    Toxicity labels from several crowd-workers for each comment in toxicity_annotated_comments.tsv. It can be joined with toxicity_annotated_comments.tsv on rev_id.\n",
    "\n",
    "rev_id: MediaWiki revision id of the edit that added the comment to a talk page (i.e. discussion). <br>\n",
    "worker_id: Anonymized crowd-worker id.<br>\n",
    "toxicity_score: Categorical variable ranging from very toxic (-2), to neutral (0), to very healthy (2). <br>\n",
    "toxicity: Indicator variable for whether the worker thought the comment is toxic. The annotation takes on the value 1 if the worker considered the comment toxic (i.e worker gave a toxicity_score less than 0) and value 0 if the worker considered the comment neutral or healthy (i.e worker gave a toxicity_score greater or equal to 0). Takes on values in {0, 1}.\n",
    "\n",
    "My notes:\n",
    "Things to explore is how many people rated each thing? The paper said 10, but I would like to confirm this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_worker_demographics = pd.read_csv(\"toxicity_worker_demographics.tsv\"\\\n",
    "                                          , sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My comment:\n",
    "    This isn't really one we'll be using until much later, if/when we decide we're doing a perturbation using demographic data. Would first want to check to see what kind of correlations might/do exist between gender/rating and see how they rate comments about women, for example.\n",
    "    \n",
    "\n",
    "Copied from documentation:\n",
    "\n",
    "Schema for {attack/aggression/toxicity}_worker_demographics.tsv\n",
    "Demographic information about the crowdworkers. This information was obtained by an optional demographic survey administered after the labelling task. It is meant to be joined with {attack/aggression/toxicity}_annotations.tsv on worker_id. Some fields may be blank if left unanswered.\n",
    "\n",
    "worker_id: Anonymized crowd-worker id. <br>\n",
    "gender: The gender of the crowd-worker. Takes a value in {'male', 'female', and 'other'}. <br>\n",
    "english_first_language: Does the crowd-worker describe English as their first language. Takes a value in {0, 1}.<br>\n",
    "age_group: The age group of the crowd-worker. Takes on values in {'Under 18', '18-30', '30-45', '45-60', 'Over 60'}.<br>\n",
    "education: The highest education level obtained by the crowd-worker. Takes on values in {'none', 'some', 'hs', 'bachelors', 'masters', 'doctorate', 'professional'}. Here 'none' means no schooling, some means 'some schooling', 'hs' means high school completion, and the remaining terms indicate completion of the corresponding degree type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_worker_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stuff I tried that didn't work and I might want later\n",
    "\n",
    "# This code is copy/pasted from https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/unintended_ml_bias/Dataset_bias_analysis.ipynb\n",
    "# grouped_ds = annotations.groupby('rev_id')['toxicity'].mean()\n",
    "# df = comments\n",
    "# df['toxic'] = annotations.groupby('rev_id')['toxicity'].mean() # > 0.5\n",
    "\n",
    "grouped_annotations = annotations.groupby('rev_id',as_index=False)['toxicity'].mean()\n",
    "joined_tox = grouped_annotations.join(comments, lsuffix='rev_id', rsuffix='rev_id', how='left', sort=True) \n",
    "\n",
    "# Stuff I might want later\n",
    "\n",
    "# # remove newline and tab tokens\n",
    "# comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "# comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "# comments['length'] = comments['comment'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_tox.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
