{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is basically the same as random_perturbations_logdi except generalized to multiple p's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import model_bias_analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina' # comment this out if higher resolution is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to always check that these files look the way you want before you run this! see data-di-with-gay to regenerate\n",
    "\n",
    "joined_tox = pd.read_csv('joined_tox.csv')\n",
    "train_comments = pd.read_csv('train_comments.csv')\n",
    "test_comments = pd.read_csv('test_comments.csv')\n",
    "madlibs_terms = ['gay', 'homosexual', 'straight', 'black', 'white', 'american', 'jewish', 'old']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates perturbations\n",
    "# Basically, creates num_perturbations number of arrays. \n",
    "# Each array is the length of the train_comments\n",
    "# Each item in the array is 0/1 and is PROBABILITY_FLIP flipped from the true value of binary toxicity \n",
    "\n",
    "def generate_perturbation_on_training(train_comments, num_perturbations, probability_flip):\n",
    "    print(\"probflip\", probability_flip)\n",
    "    \n",
    "    list_perturbation = []\n",
    "    rand = []\n",
    "    tox_tmp = []\n",
    "    num_flipped = 0\n",
    "    \n",
    "    length = len(train_comments.binary_tox.values)\n",
    "    for j in range(num_perturbations):\n",
    "        rand = np.random.random(length) # generate a random number (between 0 and 1) for each comment\n",
    "        tox_tmp = np.copy(train_comments.binary_tox.values) # np.copy(tox_np)\n",
    "        for i in range(length):\n",
    "            if rand[i] >= probability_flip: # if random number is greater than 0.5, replace value in array with a random integer from [0, 1]\n",
    "                tox_tmp[i] = np.random.randint(2)\n",
    "                num_flipped += 1\n",
    "        list_perturbation.append(tox_tmp)\n",
    "\n",
    "    # each item in list_perturbation is a list of 0s and 1s that correspond to the new binary_tox of each variable\n",
    "    print(num_flipped)\n",
    "    return list_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_comments, list_perturbations_training, test_comments, NUM_PERTURBATIONS):\n",
    "\n",
    "    # This trains a classifier on n different perturbed datsets\n",
    "    d={}\n",
    "    for x in range(NUM_PERTURBATIONS):\n",
    "        d[\"clf{0}\".format(x)] = Pipeline([\n",
    "            ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "            ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "            ('clf', LogisticRegression()),\n",
    "        ])\n",
    "        d[\"clf{0}\".format(x)] = d[\"clf{0}\".format(x)].\\\n",
    "                                    fit(train_comments['comment'], list_perturbations_training[x])\n",
    "        d[\"auc{0}\".format(x)] = roc_auc_score(test_comments['binary_tox'], \\\n",
    "                                    d[\"clf{0}\".format(x)].predict_proba(test_comments['comment'])[:, 1])\n",
    "        print('x Test ROC AUC: %.5f' %d[\"auc{0}\".format(x)])\n",
    "        \n",
    "    # Once a classifier is trained, this goes to the test data and creates predictions on test data\n",
    "    perturbed_predictions = [] # list, each item is array of predictions. element 0 is 0th perturbation and \n",
    "    # predictions based on that.\n",
    "    # each item in the array is a column that indicates 0/1 for predicted not-toxic/toxic\n",
    "\n",
    "    for i in range(NUM_PERTURBATIONS):\n",
    "        perturbed_predictions.append(d[\"clf{0}\".format(i)].predict(test_comments['comment']))\n",
    "                                                                                 \n",
    "    return perturbed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logDI(df, labels_col, terms):\n",
    "    # labels_col is true/false of the comment being classified as toxic. ('binary_tox' I believe)\n",
    "    # terms should be the array of top 8 terms\n",
    "    \n",
    "    logDI_arr = np.empty(((len(terms)), len(terms)))\n",
    "    \n",
    "    for i in range(len(terms)):\n",
    "        for j in range(len(terms)):\n",
    "            # print(\"******\", i)\n",
    "            logDI_arr[i, j] =  (math.log(len(df[(df[terms[i]]==True) & (df[labels_col]==True)]) / len(df[df[terms[i]]==True])) \\\n",
    "                            - math.log(len(df[(df[terms[j]]==True) & (df[labels_col]==True)]) / len(df[df[terms[j]]==True])))**2\n",
    "                \n",
    "    logDI = logDI_arr.sum() / 2\n",
    "    return logDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_avg_logDI(list_perturbations, df, madlibs_terms):\n",
    "    # find logDI for each item in list_perturbations_training\n",
    "    logs = []\n",
    "    for i in range(len(list_perturbations)):\n",
    "        namecol = 'perturbation' + str(i)\n",
    "        df[namecol] = list_perturbations[i]\n",
    "        logs.append(logDI(df, namecol, madlibs_terms))\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probflip 0.05\n",
      "908637\n",
      "x Test ROC AUC: 0.57471\n",
      "x Test ROC AUC: 0.57982\n"
     ]
    }
   ],
   "source": [
    "p_vals = [0.05, 0.1, 0.15, 0.20, 0.25, 0.3, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.7, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "logs_training = [] # means\n",
    "logs_test = [] # means\n",
    "list_training = [] # perturbations\n",
    "list_test = [] # perturbations\n",
    "\n",
    "xs_all_logs = [] # corresponds to below\n",
    "all_logs_training = [] # not means\n",
    "all_logs_test = [] # not means\n",
    "logs_training_list = [] # temp\n",
    "logs_test_list = [] # temp\n",
    "\n",
    "NUM_PERTURBATIONS = 10 # This is the number of perturbations we're doing on EACH p-val below\n",
    "for i in range(len(p_vals)):\n",
    "    probability_flip = p_vals[i]\n",
    "    \n",
    "    list_perturbations_training = generate_perturbation_on_training(train_comments, NUM_PERTURBATIONS, probability_flip)\n",
    "    list_training.append(list_perturbations_training)\n",
    "    \n",
    "    logs_training_list = find_avg_logDI(list_perturbations_training, train_comments, madlibs_terms)\n",
    "    logs_training.append(np.mean(logs_training_list))\n",
    "    all_logs_training.append(logs_training_list)\n",
    "    xs_all_logs.append([p_vals[i]]*NUM_PERTURBATIONS)\n",
    "    \n",
    "    perturbed_predictions = train_and_predict(train_comments, list_perturbations_training, test_comments, NUM_PERTURBATIONS)\n",
    "    list_test.append(perturbed_predictions)\n",
    "    \n",
    "    logs_test_list = find_avg_logDI(perturbed_predictions, test_comments, madlibs_terms)\n",
    "    logs_test.append(np.mean(logs_test_list))\n",
    "    all_logs_test.append(logs_test_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(logs_training, logs_test, \"ro\")\n",
    "plt.title(\"Relationship between Training and Test Data log DI for various ps\")\n",
    "plt.xlabel(\"Training Data log DI of two terms\")\n",
    "plt.ylabel(\"Test Data log DI of two terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(p_vals)):\n",
    "    p_vals[i] = 1 - p_vals[i]\n",
    "print(p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_vals, logs_training, \"ro\")\n",
    "plt.title(\"Map of ps to training logs\")\n",
    "plt.xlabel(\"p's\")\n",
    "plt.ylabel(\"log DI of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_vals, logs_training)\n",
    "plt.title(\"Map of ps to training logs\")\n",
    "plt.xlabel(\"p's\")\n",
    "plt.ylabel(\"log DI of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_vals, logs_test, \"ro\")\n",
    "plt.title(\"Map of ps to test logs\")\n",
    "plt.xlabel(\"p's\")\n",
    "plt.ylabel(\"log DI of test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_vals, logs_test)\n",
    "plt.title(\"Map of ps to test logs\")\n",
    "plt.xlabel(\"p's\")\n",
    "plt.ylabel(\"log DI of test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xs_all_logs = []\n",
    "# print(ys_all_logs)\n",
    "#for i in range(len(ys_all_logs)):\n",
    "#    xs_all_logs.append(ys_all_logs[i])\n",
    "#print(xs_all_logs)\n",
    "#print(all_logs_training)\n",
    "\n",
    "xs_all_logs = np.concatenate(xs_all_logs, axis=None)\n",
    "#print(xs_all_logs)\n",
    "#print(len(xs_all_logs))\n",
    "print(xs_all_logs)\n",
    "for i in range(len(xs_all_logs)):\n",
    "    xs_all_logs[i] = 1 - xs_all_logs[i]\n",
    "print(xs_all_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logs_training = np.concatenate(all_logs_training, axis = None)\n",
    "print(all_logs_training)\n",
    "print(xs_all_logs)\n",
    "all_logs_test = np.concatenate(all_logs_test, axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "# blue is training\n",
    "plt.plot(xs_all_logs, all_logs_training, \"bo\", alpha = .3, label = \"Training Mean Value\")\n",
    "plt.plot(p_vals, logs_training, \"blue\", label = \"Training\")\n",
    "\n",
    "# red will be test\n",
    "plt.plot(p_vals, logs_test, \"red\", alpha = .3, label = \"Test Mean Value\")\n",
    "plt.plot(xs_all_logs, all_logs_test, \"ro\", label = \"Test\")\n",
    "\n",
    "plt.plot()\n",
    "plt.title(\"Disparate Impact of Training Data Falls as p Increases\", fontsize=20)\n",
    "plt.xlabel(\"Percentage (p) of comments with synthetic labels\", fontsize=20)\n",
    "plt.ylabel(\"LogDI of training data\", fontsize=20)\n",
    "plt.savefig('DI_tr_data_p_incr.png', bbox_inches='tight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(ys_all_logs, all_logs_test)\n",
    "# plt.title(\"Map of not-avg ps to test logs\")\n",
    "# plt.xlabel(\"p's\")\n",
    "# plt.ylabel(\"log DI of test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perturbed_training = pd.DataFrame(list_training)\n",
    "df_perturbed_training = df_perturbed_training.T\n",
    "df_perturbed_training.columns=p_vals\n",
    "df_perturbed_training.to_csv(\"perturbed_training2.csv\", index=False)\n",
    "df_perturbed_test = pd.DataFrame(list_test)\n",
    "df_perturbed_test = df_perturbed_test.T\n",
    "df_perturbed_test.columns=p_vals\n",
    "df_perturbed_training.to_csv(\"perturbed_test2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perturbed_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(p_vals, logs_test, \"ro\", label='test Dataset')\n",
    "plt.plot(p_vals, logs_training, \"o\", label='Training Dataset')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
